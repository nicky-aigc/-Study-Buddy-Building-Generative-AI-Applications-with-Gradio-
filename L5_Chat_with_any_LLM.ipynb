{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5753a0c",
   "metadata": {},
   "source": [
    "# L5: Chat with any LLM! üí¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a3724",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6fa00-6bd1-4839-bcaf-8bae9267ee79",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095da8fe-24aa-4dc7-8e08-aa2f949ae21f",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# SmolLM-360M-Instruct\n",
    "client = InferenceClient(\n",
    "    \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
    "    token=hf_api_key,\n",
    ")\n",
    "\n",
    "# # non-streamable chat_completion\n",
    "# message =client.chat_completion(\n",
    "# \tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "# \tmax_tokens=500,\n",
    "# \tstream=False,\n",
    "# )\n",
    "# print(message.choices[0].message.content)\n",
    "\n",
    "# streamable chat_completion\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"},{\"role\": \"assistant\", \"content\": \"Paris, the city of light and glamour! \"},{\"role\": \"user\", \"content\": \"what about the capital of Spain?\"}],\n",
    "\tmax_tokens=50,\n",
    "\tstream=True\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6fc97",
   "metadata": {},
   "source": [
    "## Building an app to chat with any LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a3c9b",
   "metadata": {},
   "source": [
    "Here we'll be using an [HuggingFaceTB/SmolLM-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM-360M-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7065860-3c0b-490d-9e7c-22e5b79fc004",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "prompt = \"Has math been invented or discovered?\"\n",
    "client.chat_completion(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": prompt}], max_tokens=100,stream=False).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "#Back to Lesson 2, time flies!\n",
    "import gradio as gr\n",
    "def generate(input, slider):\n",
    "    # ‰ΩøÁî®Êñ∞ÁöÑÊñπÂºèÊù•Ëé∑ÂèñÊ∂àÊÅØÂÜÖÂÆπ\n",
    "    response = client.chat_completion(messages=[{\"role\": \"user\", \"content\": input}], max_tokens=slider)\n",
    "    output = response.choices[0].message.content  # ‰ΩøÁî®Êï∞ÊçÆÁ±ªÂ±ûÊÄß\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, \n",
    "                    inputs=[gr.Textbox(label=\"User input:\"), \n",
    "                            gr.Slider(label=\"Max tokens\", \n",
    "                                      value=50,  \n",
    "                                      maximum=1024, \n",
    "                                      minimum=1,\n",
    "                                      step=10)], \n",
    "                    outputs=[gr.Textbox(label=\"ChatBot output:\")])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f55e2",
   "metadata": {},
   "source": [
    "## `gr.Chatbot()`\n",
    "\n",
    "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
    "- Define your `fn` to take in a `gr.Chatbot()` object.  \n",
    "  - Within your defined `fn` function, append a tuple (or a list) containing the user message and the LLM's response:\n",
    "`chatbot_object.append( (user_message, llm_message) )`\n",
    "\n",
    "- Include the chatbot object in both the inputs and the outputs of the app.\n",
    "- Use Qingyan's glms API to support the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43beebb7-40a6-4af5-a701-882821b6ed36",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646d777-c211-4d31-9426-7b5d78b533ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "# As the inference is not available, I have commented out the code.\n",
    "# def format_chat_prompt(message, chat_history):\n",
    "#     prompt = \"\"\n",
    "#     for turn in chat_history:\n",
    "#         user_message, bot_message = turn\n",
    "#         prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "#     prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "#     return prompt\n",
    "import requests, json\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "\n",
    "# SmolLM-360M-Instruct\n",
    "client = InferenceClient(\n",
    "    \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
    "    token=hf_api_key,\n",
    ")\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    chat_history.append((message, None))\n",
    "    \n",
    "    try:\n",
    "\n",
    "        messages = [{\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": m} for i, (m, _) in enumerate(chat_history)]\n",
    "\n",
    "        response= client.chat_completion(messages=messages,\n",
    "                                        max_tokens=128,\n",
    "                                        stream=False\n",
    "                                        )\n",
    "        bot_message = response.choices[0].message.content\n",
    "        chat_history[-1] = (message, bot_message)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        chat_history[-1] = (message, f\"Error: {str(e)}\")\n",
    "    \n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b8de8",
   "metadata": {},
   "source": [
    "### Adding other advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fff81-a3d1-4cb8-8d6e-d152ab39065a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "\n",
    "# SmolLM-360M-Instruct\n",
    "client = InferenceClient(\n",
    "    \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
    "    token=hf_api_key,\n",
    ")\n",
    "\n",
    "def respond(message, chat_history,instruction=\"You are a helpful assistant\", temperature=0.7, top_p=0.9,stop=None,stream=False,max_tokens=128):\n",
    "    chat_history.append((message, None))\n",
    "    \n",
    "    try:\n",
    "\n",
    "        messages = [{\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": m} for i, (m, _) in enumerate(chat_history)]\n",
    "        message_zero=[{\"role\":\"system\",\"content\":\"You are a helpful assistant and you always provides short and concise response.\"}]\n",
    "        messages=message_zero+messages\n",
    "\n",
    "        response= client.chat_completion(messages=messages,\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        stream=stream,\n",
    "                                        temperature=temperature,\n",
    "                                        top_p=top_p,\n",
    "                                        stop=stop\n",
    "                \n",
    "                                        )\n",
    "        bot_message = response.choices[0].message.content\n",
    "        chat_history[-1] = (message, bot_message)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        chat_history[-1] = (message, f\"Error: {str(e)}\")\n",
    "    \n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee9bc5-fce7-44b1-af2a-e69bc7c598b6",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "- If your LLM can provide its tokens one at a time in a stream, you can accumulate those tokens in the chatbot object.\n",
    "- The `for` loop in the following function goes through all the tokens that are in the stream and appends them to the most recent conversational turn in the chatbot's message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "700eb3bc-b63a-4ccb-94c4-70ec2e54bcda",
   "metadata": {
    "height": 438
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7883\n",
      "Running on public URL: https://3b836e5eee11b486fa.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3b836e5eee11b486fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def respond(message, chat_history,system=\"You are a helpful assistant\", temperature=0.7, top_p=0.9,stop=[\"\\n\"],stream=True,max_tokens=128):\n",
    "    chat_history.append((message, None))\n",
    "    \n",
    "    try:\n",
    "\n",
    "        messages = [{\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": m} for i, (m, _) in enumerate(chat_history)]\n",
    "        message_zero=[{\"role\":\"system\",\"content\":system}]\n",
    "        messages=message_zero+messages\n",
    "\n",
    "        response= client.chat_completion(messages=messages,\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        stream=stream,\n",
    "                                        temperature=temperature,\n",
    "                                        top_p=top_p,\n",
    "                                        stop=stop\n",
    "                \n",
    "                                        )\n",
    "        bot_message = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                bot_message += chunk.choices[0].delta.content\n",
    "                chat_history[-1] = (message, bot_message)\n",
    "                yield \"\", chat_history\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        chat_history[-1] = (message, f\"Error: {str(e)}\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system,temperature], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system,temperature], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a51a07",
   "metadata": {},
   "source": [
    "Notice, in the cell above, you have used `demo.queue().launch()` instead of `demo.launch()`. \"queue\" helps you to boost up the performance for your demo. You can read [setting up a demo for maximum performance](https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf9b3a-4202-4e3a-9c6b-941fa1290ab8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
